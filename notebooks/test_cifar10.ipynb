{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "\n",
    "os.chdir(\"../../..\")\n",
    "\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>sku</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>...</th>\n",
       "      <th>football_gold_cup</th>\n",
       "      <th>beer_capital</th>\n",
       "      <th>music_fest</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>month</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>avg_volume_by_sku</th>\n",
       "      <th>avg_volume_by_agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Agency_25</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>0.5076</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>492612703</td>\n",
       "      <td>718394219</td>\n",
       "      <td>25.845238</td>\n",
       "      <td>1264.162234</td>\n",
       "      <td>1152.473405</td>\n",
       "      <td>111.688829</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>8.835008</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.678062</td>\n",
       "      <td>1225.306376</td>\n",
       "      <td>99.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>Agency_29</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>8.7480</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>498567142</td>\n",
       "      <td>762225057</td>\n",
       "      <td>27.584615</td>\n",
       "      <td>1316.098485</td>\n",
       "      <td>1296.804924</td>\n",
       "      <td>19.293561</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.465966</td>\n",
       "      <td>177</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2.168825</td>\n",
       "      <td>1634.434615</td>\n",
       "      <td>11.397086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19532</th>\n",
       "      <td>Agency_47</td>\n",
       "      <td>SKU_01</td>\n",
       "      <td>4.9680</td>\n",
       "      <td>2013-09-01</td>\n",
       "      <td>454252482</td>\n",
       "      <td>789624076</td>\n",
       "      <td>30.665957</td>\n",
       "      <td>1269.250000</td>\n",
       "      <td>1266.490490</td>\n",
       "      <td>2.759510</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.217413</td>\n",
       "      <td>322</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1.603017</td>\n",
       "      <td>2625.472644</td>\n",
       "      <td>48.295650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Agency_53</td>\n",
       "      <td>SKU_07</td>\n",
       "      <td>21.6825</td>\n",
       "      <td>2013-10-01</td>\n",
       "      <td>480693900</td>\n",
       "      <td>791658684</td>\n",
       "      <td>29.197727</td>\n",
       "      <td>1193.842373</td>\n",
       "      <td>1128.124395</td>\n",
       "      <td>65.717978</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>beer_capital</td>\n",
       "      <td>-</td>\n",
       "      <td>5.504745</td>\n",
       "      <td>240</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>3.076505</td>\n",
       "      <td>38.529107</td>\n",
       "      <td>2511.035175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>Agency_17</td>\n",
       "      <td>SKU_02</td>\n",
       "      <td>960.5520</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>515468092</td>\n",
       "      <td>871204688</td>\n",
       "      <td>23.608120</td>\n",
       "      <td>1338.334248</td>\n",
       "      <td>1232.128069</td>\n",
       "      <td>106.206179</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>music_fest</td>\n",
       "      <td>7.935699</td>\n",
       "      <td>259</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>6.867508</td>\n",
       "      <td>2143.677462</td>\n",
       "      <td>396.022140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>Agency_05</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>1184.6535</td>\n",
       "      <td>2014-02-01</td>\n",
       "      <td>425528909</td>\n",
       "      <td>734443953</td>\n",
       "      <td>28.668254</td>\n",
       "      <td>1369.556376</td>\n",
       "      <td>1161.135214</td>\n",
       "      <td>208.421162</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>15.218151</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>7.077206</td>\n",
       "      <td>1566.643589</td>\n",
       "      <td>1881.866367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19204</th>\n",
       "      <td>Agency_11</td>\n",
       "      <td>SKU_05</td>\n",
       "      <td>5.5593</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>623319783</td>\n",
       "      <td>1049868815</td>\n",
       "      <td>31.915385</td>\n",
       "      <td>1922.486644</td>\n",
       "      <td>1651.307674</td>\n",
       "      <td>271.178970</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>14.105636</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "      <td>8</td>\n",
       "      <td>1.715472</td>\n",
       "      <td>1385.225478</td>\n",
       "      <td>109.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8781</th>\n",
       "      <td>Agency_48</td>\n",
       "      <td>SKU_04</td>\n",
       "      <td>4275.1605</td>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>509281531</td>\n",
       "      <td>892192092</td>\n",
       "      <td>26.767857</td>\n",
       "      <td>1761.258209</td>\n",
       "      <td>1546.059670</td>\n",
       "      <td>215.198539</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>music_fest</td>\n",
       "      <td>12.218455</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8.360577</td>\n",
       "      <td>1757.950603</td>\n",
       "      <td>1925.272108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>Agency_07</td>\n",
       "      <td>SKU_21</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>544203593</td>\n",
       "      <td>761469815</td>\n",
       "      <td>28.987755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>-18.420681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2418.719550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12084</th>\n",
       "      <td>Agency_21</td>\n",
       "      <td>SKU_03</td>\n",
       "      <td>46.3608</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>589969396</td>\n",
       "      <td>940912941</td>\n",
       "      <td>32.478910</td>\n",
       "      <td>1675.922116</td>\n",
       "      <td>1413.571789</td>\n",
       "      <td>262.350327</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>15.654088</td>\n",
       "      <td>181</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>3.836454</td>\n",
       "      <td>2034.293024</td>\n",
       "      <td>109.381800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          agency     sku     volume       date  industry_volume  soda_volume  \\\n",
       "291    Agency_25  SKU_03     0.5076 2013-01-01        492612703    718394219   \n",
       "871    Agency_29  SKU_02     8.7480 2015-01-01        498567142    762225057   \n",
       "19532  Agency_47  SKU_01     4.9680 2013-09-01        454252482    789624076   \n",
       "2089   Agency_53  SKU_07    21.6825 2013-10-01        480693900    791658684   \n",
       "9755   Agency_17  SKU_02   960.5520 2015-03-01        515468092    871204688   \n",
       "7561   Agency_05  SKU_03  1184.6535 2014-02-01        425528909    734443953   \n",
       "19204  Agency_11  SKU_05     5.5593 2017-08-01        623319783   1049868815   \n",
       "8781   Agency_48  SKU_04  4275.1605 2013-03-01        509281531    892192092   \n",
       "2540   Agency_07  SKU_21     0.0000 2015-10-01        544203593    761469815   \n",
       "12084  Agency_21  SKU_03    46.3608 2017-04-01        589969396    940912941   \n",
       "\n",
       "       avg_max_temp  price_regular  price_actual    discount  ...  \\\n",
       "291       25.845238    1264.162234   1152.473405  111.688829  ...   \n",
       "871       27.584615    1316.098485   1296.804924   19.293561  ...   \n",
       "19532     30.665957    1269.250000   1266.490490    2.759510  ...   \n",
       "2089      29.197727    1193.842373   1128.124395   65.717978  ...   \n",
       "9755      23.608120    1338.334248   1232.128069  106.206179  ...   \n",
       "7561      28.668254    1369.556376   1161.135214  208.421162  ...   \n",
       "19204     31.915385    1922.486644   1651.307674  271.178970  ...   \n",
       "8781      26.767857    1761.258209   1546.059670  215.198539  ...   \n",
       "2540      28.987755       0.000000      0.000000    0.000000  ...   \n",
       "12084     32.478910    1675.922116   1413.571789  262.350327  ...   \n",
       "\n",
       "       football_gold_cup  beer_capital  music_fest discount_in_percent  \\\n",
       "291                    -             -           -            8.835008   \n",
       "871                    -             -           -            1.465966   \n",
       "19532                  -             -           -            0.217413   \n",
       "2089                   -  beer_capital           -            5.504745   \n",
       "9755                   -             -  music_fest            7.935699   \n",
       "7561                   -             -           -           15.218151   \n",
       "19204                  -             -           -           14.105636   \n",
       "8781                   -             -  music_fest           12.218455   \n",
       "2540                   -             -           -            0.000000   \n",
       "12084                  -             -           -           15.654088   \n",
       "\n",
       "      timeseries time_idx month log_volume avg_volume_by_sku  \\\n",
       "291          228        0     1  -0.678062       1225.306376   \n",
       "871          177       24     1   2.168825       1634.434615   \n",
       "19532        322        8     9   1.603017       2625.472644   \n",
       "2089         240        9    10   3.076505         38.529107   \n",
       "9755         259       26     3   6.867508       2143.677462   \n",
       "7561          21       13     2   7.077206       1566.643589   \n",
       "19204         17       55     8   1.715472       1385.225478   \n",
       "8781         151        2     3   8.360577       1757.950603   \n",
       "2540         300       33    10 -18.420681          0.000000   \n",
       "12084        181       51     4   3.836454       2034.293024   \n",
       "\n",
       "      avg_volume_by_agency  \n",
       "291              99.650400  \n",
       "871              11.397086  \n",
       "19532            48.295650  \n",
       "2089           2511.035175  \n",
       "9755            396.022140  \n",
       "7561           1881.866367  \n",
       "19204           109.699200  \n",
       "8781           1925.272108  \n",
       "2540           2418.719550  \n",
       "12084           109.381800  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "data = get_stallion_data()\n",
    "\n",
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "\n",
    "# we want to encode special days as one variable and thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>avg_population_2017</th>\n",
       "      <th>avg_yearly_household_income_2017</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>avg_volume_by_sku</th>\n",
       "      <th>avg_volume_by_agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21000.000000</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.00000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1492.403982</td>\n",
       "      <td>5.439214e+08</td>\n",
       "      <td>8.512000e+08</td>\n",
       "      <td>28.612404</td>\n",
       "      <td>1451.536344</td>\n",
       "      <td>1267.347450</td>\n",
       "      <td>184.374146</td>\n",
       "      <td>1.045065e+06</td>\n",
       "      <td>151073.494286</td>\n",
       "      <td>10.574884</td>\n",
       "      <td>174.50000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>2.464118</td>\n",
       "      <td>1492.403982</td>\n",
       "      <td>1492.403982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2711.496882</td>\n",
       "      <td>6.288022e+07</td>\n",
       "      <td>7.824340e+07</td>\n",
       "      <td>3.972833</td>\n",
       "      <td>683.362417</td>\n",
       "      <td>587.757323</td>\n",
       "      <td>257.469968</td>\n",
       "      <td>9.291926e+05</td>\n",
       "      <td>50409.593114</td>\n",
       "      <td>9.590813</td>\n",
       "      <td>101.03829</td>\n",
       "      <td>17.318515</td>\n",
       "      <td>8.178218</td>\n",
       "      <td>1051.790829</td>\n",
       "      <td>1328.239698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.130518e+08</td>\n",
       "      <td>6.964015e+08</td>\n",
       "      <td>16.731034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3121.690141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.227100e+04</td>\n",
       "      <td>90240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-18.420681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.272388</td>\n",
       "      <td>5.090553e+08</td>\n",
       "      <td>7.890880e+08</td>\n",
       "      <td>25.374816</td>\n",
       "      <td>1311.547158</td>\n",
       "      <td>1178.365653</td>\n",
       "      <td>54.935108</td>\n",
       "      <td>6.018900e+04</td>\n",
       "      <td>110057.000000</td>\n",
       "      <td>3.749628</td>\n",
       "      <td>87.00000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>2.112923</td>\n",
       "      <td>932.285496</td>\n",
       "      <td>113.420250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>158.436000</td>\n",
       "      <td>5.512000e+08</td>\n",
       "      <td>8.649196e+08</td>\n",
       "      <td>28.479272</td>\n",
       "      <td>1495.174592</td>\n",
       "      <td>1324.695705</td>\n",
       "      <td>138.307225</td>\n",
       "      <td>1.232242e+06</td>\n",
       "      <td>131411.000000</td>\n",
       "      <td>8.948990</td>\n",
       "      <td>174.50000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>5.065351</td>\n",
       "      <td>1402.305264</td>\n",
       "      <td>1730.529771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1774.793475</td>\n",
       "      <td>5.893715e+08</td>\n",
       "      <td>9.005551e+08</td>\n",
       "      <td>31.568405</td>\n",
       "      <td>1725.652080</td>\n",
       "      <td>1517.311427</td>\n",
       "      <td>272.298630</td>\n",
       "      <td>1.729177e+06</td>\n",
       "      <td>206553.000000</td>\n",
       "      <td>15.647058</td>\n",
       "      <td>262.00000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>7.481439</td>\n",
       "      <td>2195.362302</td>\n",
       "      <td>2595.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22526.610000</td>\n",
       "      <td>6.700157e+08</td>\n",
       "      <td>1.049869e+09</td>\n",
       "      <td>45.290476</td>\n",
       "      <td>19166.625000</td>\n",
       "      <td>4925.404000</td>\n",
       "      <td>19166.625000</td>\n",
       "      <td>3.137874e+06</td>\n",
       "      <td>247220.000000</td>\n",
       "      <td>226.740147</td>\n",
       "      <td>349.00000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>10.022453</td>\n",
       "      <td>4332.363750</td>\n",
       "      <td>5884.717375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             volume  industry_volume   soda_volume  avg_max_temp  \\\n",
       "count  21000.000000     2.100000e+04  2.100000e+04  21000.000000   \n",
       "mean    1492.403982     5.439214e+08  8.512000e+08     28.612404   \n",
       "std     2711.496882     6.288022e+07  7.824340e+07      3.972833   \n",
       "min        0.000000     4.130518e+08  6.964015e+08     16.731034   \n",
       "25%        8.272388     5.090553e+08  7.890880e+08     25.374816   \n",
       "50%      158.436000     5.512000e+08  8.649196e+08     28.479272   \n",
       "75%     1774.793475     5.893715e+08  9.005551e+08     31.568405   \n",
       "max    22526.610000     6.700157e+08  1.049869e+09     45.290476   \n",
       "\n",
       "       price_regular  price_actual      discount  avg_population_2017  \\\n",
       "count   21000.000000  21000.000000  21000.000000         2.100000e+04   \n",
       "mean     1451.536344   1267.347450    184.374146         1.045065e+06   \n",
       "std       683.362417    587.757323    257.469968         9.291926e+05   \n",
       "min         0.000000  -3121.690141      0.000000         1.227100e+04   \n",
       "25%      1311.547158   1178.365653     54.935108         6.018900e+04   \n",
       "50%      1495.174592   1324.695705    138.307225         1.232242e+06   \n",
       "75%      1725.652080   1517.311427    272.298630         1.729177e+06   \n",
       "max     19166.625000   4925.404000  19166.625000         3.137874e+06   \n",
       "\n",
       "       avg_yearly_household_income_2017  discount_in_percent   timeseries  \\\n",
       "count                      21000.000000         21000.000000  21000.00000   \n",
       "mean                      151073.494286            10.574884    174.50000   \n",
       "std                        50409.593114             9.590813    101.03829   \n",
       "min                        90240.000000             0.000000      0.00000   \n",
       "25%                       110057.000000             3.749628     87.00000   \n",
       "50%                       131411.000000             8.948990    174.50000   \n",
       "75%                       206553.000000            15.647058    262.00000   \n",
       "max                       247220.000000           226.740147    349.00000   \n",
       "\n",
       "           time_idx    log_volume  avg_volume_by_sku  avg_volume_by_agency  \n",
       "count  21000.000000  21000.000000       21000.000000          21000.000000  \n",
       "mean      29.500000      2.464118        1492.403982           1492.403982  \n",
       "std       17.318515      8.178218        1051.790829           1328.239698  \n",
       "min        0.000000    -18.420681           0.000000              0.000000  \n",
       "25%       14.750000      2.112923         932.285496            113.420250  \n",
       "50%       29.500000      5.065351        1402.305264           1730.529771  \n",
       "75%       44.250000      7.481439        2195.362302           2595.316500  \n",
       "max       59.000000     10.022453        4332.363750           5884.717375  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 128  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(293.0088)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating predictions\n",
    "predictions = Baseline().predict(val_dataloader)\n",
    "\n",
    "# calculate baseline performance in terms of mean absolute error (MAE)\n",
    "metric = MAE()\n",
    "model = Baseline()\n",
    "for x, y in val_dataloader:\n",
    "    metric.update(model(x).prediction, y)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 13.5k\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=8,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    loss=QuantileLoss(),\n",
    "    optimizer=\"Ranger\",\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06adfb2bb0804f87a5d280a954786aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.041686938347033554\n",
      "Restoring states from the checkpoint path at /Users/abdulrahman/Documents/DTU/.lr_find_b1f08ca5-d389-4873-8572-910901654bcb.ckpt\n",
      "Restored all states from the checkpoint at /Users/abdulrahman/Documents/DTU/.lr_find_b1f08ca5-d389-4873-8572-910901654bcb.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: 0.041686938347033554\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 29.4k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: lightning_logs/lightning_logs\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 1.3 K \n",
      "3  | prescalers                         | ModuleDict                      | 256   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 3.4 K \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 8.0 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.7 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K \n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "20 | output_layer                       | Linear                          | 119   \n",
      "----------------------------------------------------------------------------------------\n",
      "29.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "29.4 K    Total params\n",
      "0.118     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e05431c9584c4b9002d1c9b1ceddce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c64400bd9a647bc9fa794b11d690e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2add4d3b13d44a24ab44c6a8a66b1c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10628c27ec604431b605212268d9dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cfaa7272ae40c7b21ca777853adafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dc3600fbf640c583317eea65d6a3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52abe218acc4ef09b43ea8a37f2936d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31d703a6f2c4128947e0fb33b322d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5157f8b077d446a997b6a86703952468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7705197fb54848ca8b0f78802b652c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba12e8307ca646a1845f64a7e34fa366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752405d3a13642ee90a7988b303c74c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18046cba55f4930b122af22bfc4434a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b5d9a535654666990cbcfe255ec4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0e243d549f44dca1232d186dedc8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4885b175d4493e9043e18808a645d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7395dd9bae694578b8e5ce8efcd6cad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdefc41c67a949698890af74a4d9f376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-16 12:39:29,267]\u001b[0m A new study created in memory with name: no-name-9e8764e0-65bb-4df7-b089-35bfbbd12a0f\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 12:45:59,022]\u001b[0m Trial 0 finished with value: 158.1295928955078 and parameters: {'gradient_clip_val': 0.0381332483749079, 'hidden_size': 28, 'dropout': 0.19147021124707825, 'hidden_continuous_size': 21, 'attention_head_size': 1, 'learning_rate': 0.0012568083295172654}. Best is trial 0 with value: 158.1295928955078.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 12:55:10,949]\u001b[0m Trial 1 finished with value: 168.5506591796875 and parameters: {'gradient_clip_val': 0.8048025702071824, 'hidden_size': 59, 'dropout': 0.224702618166819, 'hidden_continuous_size': 47, 'attention_head_size': 1, 'learning_rate': 0.007822984646096336}. Best is trial 0 with value: 158.1295928955078.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:05:57,690]\u001b[0m Trial 2 finished with value: 163.46461486816406 and parameters: {'gradient_clip_val': 0.022930189545478433, 'hidden_size': 76, 'dropout': 0.24527340266471837, 'hidden_continuous_size': 74, 'attention_head_size': 2, 'learning_rate': 0.0045268498703240284}. Best is trial 0 with value: 158.1295928955078.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:12:27,097]\u001b[0m Trial 3 finished with value: 167.410888671875 and parameters: {'gradient_clip_val': 0.06284421765033497, 'hidden_size': 48, 'dropout': 0.18814591412798648, 'hidden_continuous_size': 10, 'attention_head_size': 4, 'learning_rate': 0.06576649390088801}. Best is trial 0 with value: 158.1295928955078.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:17:13,948]\u001b[0m Trial 4 finished with value: 164.46807861328125 and parameters: {'gradient_clip_val': 0.025269724657841126, 'hidden_size': 9, 'dropout': 0.16750190096590695, 'hidden_continuous_size': 8, 'attention_head_size': 1, 'learning_rate': 0.052318266696294205}. Best is trial 0 with value: 158.1295928955078.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:24:59,111]\u001b[0m Trial 5 finished with value: 170.5184783935547 and parameters: {'gradient_clip_val': 0.07644001544554906, 'hidden_size': 56, 'dropout': 0.21120056134980814, 'hidden_continuous_size': 25, 'attention_head_size': 1, 'learning_rate': 0.011407987815503276}. Best is trial 0 with value: 158.1295928955078.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:30:19,564]\u001b[0m Trial 6 finished with value: 149.28451538085938 and parameters: {'gradient_clip_val': 0.01135231449503884, 'hidden_size': 13, 'dropout': 0.10446232367738438, 'hidden_continuous_size': 10, 'attention_head_size': 3, 'learning_rate': 0.002540799066859726}. Best is trial 6 with value: 149.28451538085938.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:35:17,697]\u001b[0m Trial 7 finished with value: 158.41175842285156 and parameters: {'gradient_clip_val': 0.09337412904131444, 'hidden_size': 12, 'dropout': 0.13603616691371131, 'hidden_continuous_size': 9, 'attention_head_size': 2, 'learning_rate': 0.009615577377713612}. Best is trial 6 with value: 149.28451538085938.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:40:52,948]\u001b[0m Trial 8 finished with value: 151.7418212890625 and parameters: {'gradient_clip_val': 0.037746470850789214, 'hidden_size': 25, 'dropout': 0.2534113281975636, 'hidden_continuous_size': 10, 'attention_head_size': 1, 'learning_rate': 0.005242327486107818}. Best is trial 6 with value: 149.28451538085938.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:44:38,167]\u001b[0m Trial 9 finished with value: 155.91554260253906 and parameters: {'gradient_clip_val': 0.11163085531923057, 'hidden_size': 8, 'dropout': 0.1541569730053139, 'hidden_continuous_size': 8, 'attention_head_size': 2, 'learning_rate': 0.049798659059943}. Best is trial 6 with value: 149.28451538085938.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 13:54:26,897]\u001b[0m Trial 10 finished with value: 150.53863525390625 and parameters: {'gradient_clip_val': 0.013649841067181078, 'hidden_size': 113, 'dropout': 0.10766635345412062, 'hidden_continuous_size': 17, 'attention_head_size': 4, 'learning_rate': 0.0010483975078424222}. Best is trial 6 with value: 149.28451538085938.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:03:51,824]\u001b[0m Trial 11 finished with value: 189.5596160888672 and parameters: {'gradient_clip_val': 0.01077433352739645, 'hidden_size': 108, 'dropout': 0.11115963685020167, 'hidden_continuous_size': 16, 'attention_head_size': 4, 'learning_rate': 0.0010098214192195522}. Best is trial 6 with value: 149.28451538085938.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:13:56,148]\u001b[0m Trial 12 finished with value: 150.77593994140625 and parameters: {'gradient_clip_val': 0.01001067797580814, 'hidden_size': 119, 'dropout': 0.10041582243876819, 'hidden_continuous_size': 15, 'attention_head_size': 3, 'learning_rate': 0.001958264706852639}. Best is trial 6 with value: 149.28451538085938.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:19:00,969]\u001b[0m Trial 13 finished with value: 145.85867309570312 and parameters: {'gradient_clip_val': 0.015116019416469844, 'hidden_size': 18, 'dropout': 0.13113512458301357, 'hidden_continuous_size': 11, 'attention_head_size': 3, 'learning_rate': 0.0022496025812181394}. Best is trial 13 with value: 145.85867309570312.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:23:41,717]\u001b[0m Trial 14 finished with value: 159.9210205078125 and parameters: {'gradient_clip_val': 0.017353040622260965, 'hidden_size': 16, 'dropout': 0.29541601303772547, 'hidden_continuous_size': 10, 'attention_head_size': 3, 'learning_rate': 0.002479986371694433}. Best is trial 13 with value: 145.85867309570312.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:28:51,151]\u001b[0m Trial 15 finished with value: 143.47711181640625 and parameters: {'gradient_clip_val': 0.017303298770085525, 'hidden_size': 20, 'dropout': 0.12947920718258366, 'hidden_continuous_size': 11, 'attention_head_size': 3, 'learning_rate': 0.0029339977866428713}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:34:03,140]\u001b[0m Trial 16 finished with value: 172.19076538085938 and parameters: {'gradient_clip_val': 0.021361330068052204, 'hidden_size': 20, 'dropout': 0.14247578173523764, 'hidden_continuous_size': 12, 'attention_head_size': 3, 'learning_rate': 0.0040908348353514475}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:39:53,681]\u001b[0m Trial 17 finished with value: 159.20462036132812 and parameters: {'gradient_clip_val': 0.03941713375661638, 'hidden_size': 36, 'dropout': 0.1325111814988493, 'hidden_continuous_size': 12, 'attention_head_size': 3, 'learning_rate': 0.0018471760915176607}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:45:01,564]\u001b[0m Trial 18 finished with value: 158.0741729736328 and parameters: {'gradient_clip_val': 0.021341461258586752, 'hidden_size': 20, 'dropout': 0.16574840958752737, 'hidden_continuous_size': 11, 'attention_head_size': 2, 'learning_rate': 0.003153840922332176}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:50:32,216]\u001b[0m Trial 19 finished with value: 153.50531005859375 and parameters: {'gradient_clip_val': 0.15609951298149582, 'hidden_size': 32, 'dropout': 0.12292896065054379, 'hidden_continuous_size': 13, 'attention_head_size': 4, 'learning_rate': 0.0016292406817375077}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 14:55:44,200]\u001b[0m Trial 20 finished with value: 148.87734985351562 and parameters: {'gradient_clip_val': 0.014963868180241148, 'hidden_size': 21, 'dropout': 0.15172710332299477, 'hidden_continuous_size': 9, 'attention_head_size': 3, 'learning_rate': 0.003277124256438001}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:01:20,579]\u001b[0m Trial 21 finished with value: 156.1766357421875 and parameters: {'gradient_clip_val': 0.014333850083378577, 'hidden_size': 21, 'dropout': 0.15635130877147096, 'hidden_continuous_size': 9, 'attention_head_size': 3, 'learning_rate': 0.0031775947201867732}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:06:20,956]\u001b[0m Trial 22 finished with value: 149.22088623046875 and parameters: {'gradient_clip_val': 0.015696010048504697, 'hidden_size': 16, 'dropout': 0.12560283573717312, 'hidden_continuous_size': 9, 'attention_head_size': 3, 'learning_rate': 0.001705959319133069}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:12:21,178]\u001b[0m Trial 23 finished with value: 174.106689453125 and parameters: {'gradient_clip_val': 0.03179073194937926, 'hidden_size': 25, 'dropout': 0.14699968190941332, 'hidden_continuous_size': 11, 'attention_head_size': 3, 'learning_rate': 0.0061369076713977235}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:18:35,463]\u001b[0m Trial 24 finished with value: 152.87362670898438 and parameters: {'gradient_clip_val': 0.016416846562573018, 'hidden_size': 37, 'dropout': 0.17443729153912732, 'hidden_continuous_size': 8, 'attention_head_size': 2, 'learning_rate': 0.00344001914636479}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:27:06,203]\u001b[0m Trial 25 finished with value: 144.21961975097656 and parameters: {'gradient_clip_val': 0.010544576501032361, 'hidden_size': 16, 'dropout': 0.12519931125591288, 'hidden_continuous_size': 9, 'attention_head_size': 4, 'learning_rate': 0.006011903365402126}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:32:02,762]\u001b[0m Trial 26 finished with value: 147.06761169433594 and parameters: {'gradient_clip_val': 0.010919569524100286, 'hidden_size': 12, 'dropout': 0.1295354598983257, 'hidden_continuous_size': 8, 'attention_head_size': 4, 'learning_rate': 0.014604111965139905}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:39:11,746]\u001b[0m Trial 27 finished with value: 154.33554077148438 and parameters: {'gradient_clip_val': 0.029291248700527246, 'hidden_size': 15, 'dropout': 0.11548773779792541, 'hidden_continuous_size': 10, 'attention_head_size': 4, 'learning_rate': 0.006234405681664244}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:44:03,614]\u001b[0m Trial 28 finished with value: 150.6253204345703 and parameters: {'gradient_clip_val': 0.020547154399751882, 'hidden_size': 10, 'dropout': 0.13842406009769145, 'hidden_continuous_size': 8, 'attention_head_size': 4, 'learning_rate': 0.004527793902363386}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:49:33,156]\u001b[0m Trial 29 finished with value: 149.7632598876953 and parameters: {'gradient_clip_val': 0.04697006134677915, 'hidden_size': 17, 'dropout': 0.11941288390624612, 'hidden_continuous_size': 11, 'attention_head_size': 4, 'learning_rate': 0.0015465904455344303}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 15:55:34,448]\u001b[0m Trial 30 finished with value: 158.5205078125 and parameters: {'gradient_clip_val': 0.028400494811291536, 'hidden_size': 26, 'dropout': 0.17708391906523577, 'hidden_continuous_size': 13, 'attention_head_size': 3, 'learning_rate': 0.0012443882560943184}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 16:00:32,068]\u001b[0m Trial 31 finished with value: 150.15499877929688 and parameters: {'gradient_clip_val': 0.010360872648120911, 'hidden_size': 12, 'dropout': 0.12920508062606106, 'hidden_continuous_size': 8, 'attention_head_size': 4, 'learning_rate': 0.012158051527184218}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n",
      "\u001b[32m[I 2023-05-16 16:05:43,972]\u001b[0m Trial 32 finished with value: 166.99343872070312 and parameters: {'gradient_clip_val': 0.01335046298436503, 'hidden_size': 11, 'dropout': 0.1199386048914628, 'hidden_continuous_size': 8, 'attention_head_size': 4, 'learning_rate': 0.015377108625456664}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m[I 2023-05-16 19:53:58,191]\u001b[0m Trial 33 finished with value: 147.16392517089844 and parameters: {'gradient_clip_val': 0.01839540343800702, 'hidden_size': 13, 'dropout': 0.13527225633471007, 'hidden_continuous_size': 9, 'attention_head_size': 4, 'learning_rate': 0.00801160794555877}. Best is trial 15 with value: 143.47711181640625.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=200,\n",
    "    max_epochs=50,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(8, 128),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d685bfd7114377445cb2c23cc6bfca7f1f2544957139cb4a27ccab868339e3e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
